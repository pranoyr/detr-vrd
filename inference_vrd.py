# -*- coding: utf-8 -*-
"""detr_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb

# Object Detection with DETR - a minimal implementation

In this notebook we show a demo of DETR (Detection Transformer), with slight differences with the baseline model in the paper.

We show how to define the model, load pretrained weights and visualize bounding box and class predictions.

Let's start with some common imports.
"""

# Commented out IPython magic to ensure Python compatibility.
from PIL import Image
import requests
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'retina'

import torch
import numpy as np
import cv2
from torch import nn
from torchvision.models import resnet50
import torchvision.transforms as T
import json
import argparse
torch.set_grad_enabled(False);

"""## DETR
Here is a minimal implementation of DETR:
"""

from main_vrd import get_args_parser
from models import build_model
import os

parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])
args = parser.parse_args()

model, criterion, postprocessors = build_model(args)
model.to("cpu")
model.eval()

model.load_state_dict(torch.load("/Users/pranoyr/Desktop/detd_vrd_model.pth", map_location=torch.device('cpu')))

with open(os.path.join(args.vrd_path, 'json_dataset', 'objects.json'), 'r') as f:
	CLASSES = json.load(f)


with open(os.path.join(args.vrd_path, 'json_dataset', 'predicates.json'), 'r') as f:
	PRD_CLASSES = json.load(f)

# root = os.path.join(self.dataset_path, 'sg_dataset', f'sg_{self.image_set}_images')

# colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
		  [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

"""DETR uses standard ImageNet normalization, and output boxes in relative image coordinates in $[x_{\text{center}}, y_{\text{center}}, w, h]$ format, where $[x_{\text{center}}, y_{\text{center}}]$ is the predicted center of the bounding box, and $w, h$ its width and height. Because the coordinates are relative to the image dimension and lies between $[0, 1]$, we convert predictions to absolute image coordinates and $[x_0, y_0, x_1, y_1]$ format for visualization purposes."""

# standard PyTorch mean-std input image normalization
transform = T.Compose([
	T.Resize(800),
	T.ToTensor(),
	T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
	x_c, y_c, w, h = x.unbind(1)
	b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
		 (x_c + 0.5 * w), (y_c + 0.5 * h)]
	return torch.stack(b, dim=1)

def set_text(draw, text, text_pos):
	font = cv2.FONT_HERSHEY_SIMPLEX
	lineThickness = 1
	font_size = 0.5
	# set some text
	# get the width and height of the text box
	(text_width, text_height) = cv2.getTextSize(text, font, font_size, lineThickness)[0]
	# set the text start position
	text_offset_x,text_offset_y = text_pos
	# make the coords of the box with a small padding of two pixels
	box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))
	cv2.rectangle(draw, box_coords[0], box_coords[1], color, cv2.FILLED)

	cv2.putText(draw, text, text_pos, font, font_size,
				(255,255,255), lineThickness, cv2.LINE_AA)

def rescale_bboxes(out_bbox, size):
	img_w, img_h = size
	b = box_cxcywh_to_xyxy(out_bbox)
	b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
	return b

"""Let's put everything together in a `detect` function:"""

def detect(im, model, transform):
	# mean-std normalize the input image (batch-size: 1)
	img = transform(im).unsqueeze(0)

	# demo model only support by default images with aspect ratio between 0.5 and 2
	# if you want to use images with an aspect ratio outside this range
	# rescale your image so that the maximum size is at most 1333 for best results
	assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'

	# propagate through the model
	outputs = model(img)

	# keep only predictions with 0.7+ confidence
	probas_prd = outputs['prd_logits'].softmax(-1)[0, :, :-1]
	keep = probas_prd.max(-1).values > 0.5
	bboxes_scaled_prd = rescale_bboxes(outputs['prd_boxes'][0, keep], im.size)


	# keep only predictions with 0.7+ confidence
	probas_sbj = outputs['sbj_logits'].softmax(-1)[0, :, :-1]
	# keep_s = probas_sbj.max(-1).values > 0.5
	# convert boxes from [0; 1] to image scales
	bboxes_scaled_sbj = rescale_bboxes(outputs['sbj_boxes'][0, keep], im.size)

  

	# keep only predictions with 0.7+ confidence
	probas_obj = outputs['obj_logits'].softmax(-1)[0, :, :-1]
	# keep_o = probas_obj.max(-1).values > 0.5
	bboxes_scaled_obj = rescale_bboxes(outputs['obj_boxes'][0, keep], im.size)
   

	print(len(probas_sbj[keep]))
	print(len(probas_prd[keep]))
	print(len(probas_obj[keep]))

	return probas_sbj[keep], probas_prd[keep], probas_obj[keep], bboxes_scaled_sbj, bboxes_scaled_prd, bboxes_scaled_obj

"""## Using DETR
To try DETRdemo model on your own image just change the URL below.
"""

# url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
im = Image.open("/Users/pranoyr/Desktop/vrd_sample/couple-people-man-girl.jpg")
img = np.array(im)
draw = img.copy()
draw_rlp = cv2.cvtColor(draw, cv2.COLOR_RGB2BGR)

scores_sbj, scores_prd, scores_obj, boxes_sbj, boxes_prd, boxes_obj = detect(im, model, transform)

"""Let's now visualize the model predictions"""

# def plot_results(pil_img, scores_sbj, scores_prd, scores_obj, boxes_sbj, boxes_prd, boxes_obj):
#     plt.figure(figsize=(16,10))
#     plt.imshow(pil_img)
#     ax = plt.gca()
	
for sbj_box, obj_box, s, p, o  \
	in zip(boxes_sbj, boxes_obj, scores_sbj, scores_prd, scores_obj):

	cl_s = s.argmax()
	cl_p = p.argmax()
	cl_o = o.argmax()
	
	sbj = CLASSES[cl_s]
	obj = CLASSES[cl_o]
	pred = PRD_CLASSES[cl_p]
	print(sbj, pred, obj)
	color = list(np.random.random(size=3) * 256)
	font = cv2.FONT_HERSHEY_SIMPLEX
	lineThickness = 1
	font_size = 0.5
	# write sbj and obj
	centr_sub = (int((sbj_box[0].item() + sbj_box[2].item())/2),
				 int((sbj_box[1].item() + sbj_box[3].item())/2))
	centr_obj = (int((obj_box[0].item() + obj_box[2].item())/2),
				 int((obj_box[1].item() + obj_box[3].item())/2))
	set_text(draw_rlp, sbj,centr_sub)
	set_text(draw_rlp, obj,centr_obj)
	# draw line conencting sbj and obj
	cv2.line(draw_rlp, centr_sub, centr_obj, color, thickness=2)
	predicate_point = (
		int((centr_sub[0] + centr_obj[0])/2), int((centr_sub[1] + centr_obj[1])/2))
	set_text(draw_rlp, pred, predicate_point)
# path = f"./results/rel-{opt.image_path.split('/')[-1]}"
cv2.imwrite("./results/a.jpg", draw_rlp)


#     for s,p,o, (xmin, ymin, xmax, ymax), c in zip(scores_sbj, scores_prd, scores_obj, boxes_sbj.tolist(), COLORS * 100):
#         ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,
#                                    fill=False, color=c, linewidth=3))
#         cl_s = s.argmax()
#         cl_p = p.argmax()
#         cl_o = o.argmax()
#         # text = f'{CLASSES[cl]}: {p[cl]:0.2f}'
#         text = f'{CLASSES[cl_s]}  {PRD_CLASSES[cl_p]} {CLASSES[cl_o]}'
#         print(text)
#         ax.text(xmin, ymin, text, fontsize=15,
#                 bbox=dict(facecolor='yellow', alpha=0.5))
#     plt.axis('off')
#     plt.show()

# print(scores_sbj.shape)
# print(boxes_sbj.shape)
# plot_results(im, scores_sbj, scores_prd, scores_obj, boxes_sbj, boxes_prd, boxes_obj)